// Copyright 2025 PingCAP, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package tici;

import "gogoproto/gogo.proto";
import "rustproto.proto";

option (gogoproto.sizer_all) = true;
option (gogoproto.marshaler_all) = true;
option (gogoproto.unmarshaler_all) = true;
option (gogoproto.goproto_unkeyed_all) = false;
option (gogoproto.goproto_unrecognized_all) = false;
option (gogoproto.goproto_sizecache_all) = false;
option (rustproto.lite_runtime_all) = true;

enum ErrorCode {
  SUCCESS = 0;
  UNKNOWN_ERROR = 1;
  INVALID_ARGUMENTS = 2;
  TRY_AGAIN = 3;

  SHARD_NOT_FOUND = 11;
  INDEX_NOT_FOUND = 12;
  WORKER_NOT_FOUND = 13;
  SHARD_NOT_SCHEDULED = 14;
  COMPACTION_NOT_FOUND = 15;
  FRAG_RANGE_MISMATCH = 16;
}

message WorkerNodeStatus {
  // TODO Need more metrics, such as disk usage, quotas, cache miss rates
  double cpu_usage = 1;
  uint64 memory_usage = 2;
}

// Reader node status information
message ReaderNodeStatus {
  // CPU usage percentage (0.0 to 100.0)
  double cpu_usage = 1;
  // Memory usage in bytes
  uint64 memory_usage = 2;
  // Number of requests processed in current heartbeat cycle
  uint32 requests_in_cycle = 3;
  // Cache hit rate percentage (0.0 to 100.0)
  double cache_hit_rate = 4;
}

// Some fields may be duplicated with ShardManifestHeader,
// however, just leave them here.
message WorkerNodeShardStatus {
  bytes start_key = 1;
  bytes end_key = 2;
  uint64 shard_id = 5;
  uint64 epoch = 6;
  uint64 seq = 7;
}

message WorkerNodeCompactionStatus { CompactFragRequest req = 1; }

message ImportWriteStatus {
  int64 table_id = 1;
  int64 index_id = 2;
  string file_prefix = 4;
}

message WorkerNodeHeartbeatRequest {
  string addr = 1;
  WorkerNodeStatus status = 2;
  repeated WorkerNodeShardStatus shards = 3;
  repeated WorkerNodeCompactionStatus compactions = 4;
  repeated ImportWriteStatus import_writes = 5;
}

message WorkerNodeHeartbeatResponse {
  ErrorCode status = 1;
  repeated AddShardRequest to_add_shards = 2;
  // We assume the `shard_id`s are unique across tables,
  // So using `shard_id` is enough here.
  repeated uint64 to_remove_shards = 3;
}

// Reader node heartbeat request
message ReaderNodeHeartbeatRequest {
  // Reader node address
  string addr = 1;
  // Reader node status
  ReaderNodeStatus status = 2;
  // TiFlash server address
  string tiflash_server_addr = 3;
  // List of cached shard IDs on this reader
  repeated uint64 cached_shard_ids = 4;
}

// Reader node heartbeat response
message ReaderNodeHeartbeatResponse {
  ErrorCode status = 1;
  // Error message, only valid when status is non-zero
  string error_message = 2;
  // List of shard headers that should be kept in cache
  repeated ShardManifestHeader shards_to_keep = 3;
  // List of shard headers that should be removed from cache
  repeated ShardManifestHeader shards_to_remove = 4;
}

// Fragment metadata primarily stores all data files under a
// ​​fragment​​
message FragProperty {
  // Total file size of fragment.
  uint64 size = 1;
  // Distinct handle count in fragment.
  uint64 count = 2;
  bytes min_handle = 3;
  bytes mid_handle = 4;
  bytes max_handle = 5;
}

message IndexSegment {
  string seg_id = 1;
  string del_suffix = 2;
}

message FragMeta {
  // Data format version​​, used for future iterative upgrades
  uint64 format_version = 1;
  // Root path of the fragment​​
  string frag_path = 2;

  bool has_deleted_set = 3;

  repeated IndexSegment segs = 4;

  FragProperty property = 5;
}

message AppendDeltaFragMetaRequest {
  int64 table_id = 1;
  int64 index_id = 2;
  uint64 shard_id = 3;
  string worker_node_addr = 4;
  repeated FragMeta frag_metas = 5;
  string last_cdc_file = 6;
}

message AppendFragMetaResponse { ErrorCode status = 1; }

message AppendBaseFragMetaRequest {
  int64 table_id = 1;
  int64 index_id = 2;
  repeated FragMeta frag_metas = 3;
}

// Key range definition
message KeyRange {
  // Inclusive lower key bound
  bytes start_key = 1;
  // Exclusive upper key bound
  bytes end_key = 2;
}

// Shard local cache information
message ShardLocalCacheInfo {
  ShardManifestHeader shard = 1;
  repeated string local_cache_addrs = 2;
}

// Request to get shard local cache information
message GetShardLocalCacheRequest {
  // Table ID to filter shards
  int64 table_id = 1;
  // Index ID to filter shards
  int64 index_id = 2;
  repeated KeyRange key_ranges = 3;
  // at most `limit` shards can be returned
  int32 limit = 4;
}

// Response containing shard local cache information
message GetShardLocalCacheResponse {
  int32 status = 1;
  repeated ShardLocalCacheInfo shard_local_cache_infos = 2;
}

message DebugGetShardManifestRequest {
  int64 table_id = 1;
  int64 index_id = 2;
  KeyRange key_range = 3;
}

message DebugGetShardManifestResponse {
  int32 status = 1;
  repeated FragMeta frag_metas = 2;
  uint64 shard_id = 3;
  uint64 seq = 4;
}

service MetaService {
  // Maintains heartbeat from worker nodes(such as writers, compactors) to meta
  // service.
  rpc WorkerNodeHeartbeat(WorkerNodeHeartbeatRequest)
      returns (WorkerNodeHeartbeatResponse);

  // Maintains heartbeat from reader nodes to meta service.
  rpc ReaderNodeHeartbeat(ReaderNodeHeartbeatRequest)
      returns (ReaderNodeHeartbeatResponse);

  // -----------------------------------------------------------------------------
  // APIs for create indexes

  // CreateIndex creates a new index
  rpc CreateIndex(CreateIndexRequest) returns (CreateIndexResponse);

  // DropIndex removes an existing index
  rpc DropIndex(DropIndexRequest) returns (DropIndexResponse);

  // GetIndexProgress retrieves the current progress of an index build
  rpc GetIndexProgress(GetIndexProgressRequest)
      returns (GetIndexProgressResponse);

  // -----------------------------------------------------------------------------

  rpc AppendDeltaFragMeta(AppendDeltaFragMetaRequest)
      returns (AppendFragMetaResponse);

  // TODO: this method should be replace by `FinishPartitionBuild`
  /// Append base fragment metadata, used for baseline shards
  /// MetaService is responsible for
  /// 1. Find the overlapped Shards for the given table_id, index_id and
  /// key_range
  /// 2. Append the frags to all the overlapped shards' baseline frags
  rpc AppendBaseFragMeta(AppendBaseFragMetaRequest)
      returns (AppendFragMetaResponse);

  // Get shard local cache information
  rpc GetShardLocalCacheInfo(GetShardLocalCacheRequest)
      returns (GetShardLocalCacheResponse);

  // Debug usage: get shard manifest from meta service writer
  rpc DebugGetShardManifest(DebugGetShardManifestRequest)
      returns (DebugGetShardManifestResponse);

  rpc FinishCompactFragments(FinishCompactFragRequest)
      returns (FinishCompactFragResponse);

  // -----------------------------------------------------------------------------
  // APIs for importing data

  // Get a cloud storage prefix for importing data.
  // This will create a new import job with a unique job_id and a cloud storage
  // prefix for the import data. The caller should generate files under the
  // cloud storage prefix and call `FinishImportPartitionUpload` after uploading
  // each partition. If there is an existing running import job for the same
  // identifier, it will return the existing job_id.
  rpc GetImportStoragePrefix(GetImportStoragePrefixRequest)
      returns (GetImportStoragePrefixResponse);

  // Called by Import DXF to notify Meta Service after a partition for
  // the given index are uploaded.
  rpc FinishImportPartitionUpload(FinishImportPartitionUploadRequest)
      returns (FinishImportResponse);

  // Called by TiCI worker to notify Meta Service after a partition for
  // the given index is built.
  rpc FinishImportPartitionBuild(FinishImportPartitionBuildRequest)
      returns (FinishImportResponse);

  // Called to notify Meta Service that the whole table/index upload is
  // finished.
  rpc FinishImportIndexUpload(FinishImportIndexUploadRequest)
      returns (FinishImportResponse);

  // -----------------------------------------------------------------------------
}

enum CompactType {
  Delta = 0;
  Base = 1;
  Full = 2;
}

message CompactFragRequest {
  ShardManifestHeader shard = 1;
  int64 table_id = 2;
  int64 index_id = 3;
  uint64 seq = 4;
  uint64 timestamp = 5;
  string leader_term = 6;
  repeated FragMeta input_frags = 7;
  S3Location upload_location = 8;
  CompactType compact_type = 9;
  string worker = 10;
}

message CompactFragResponse { int32 status = 1; }

message FinishCompactFragRequest {
  string addr = 1;
  ShardManifestHeader shard = 2;
  int64 table_id = 3;
  int64 index_id = 4;
  uint64 seq = 5;
  uint64 timestamp = 6;
  string leader_term = 7;
  repeated FragMeta input_frags = 8;
  repeated FragMeta output_frags = 9;
}

message FinishCompactFragResponse { int32 status = 1; }

// WorkerService provides index creation and deletion functionality
service WorkerService {
  // Add shard for an index
  rpc AddShard(AddShardRequest) returns (AddShardResponse);
  // Compact a shard
  rpc CompactFragments(CompactFragRequest) returns (CompactFragResponse);
  // Write import data to a shard
  rpc ImportWrite(ImportWriteRequest) returns (ImportWriteResponse);
}

message ImportWriteRequest {
  uint64 task_id = 1;
  int64 table_id = 2;
  int64 index_id = 3;
  // The location of the file to be imported
  // For example, s3://bucket/prefix/filename
  string file_location = 4;
  // The location of the fragment prefix
  // For example, s3://bucket/prefix/
  string frag_prefix = 5;
}

message ImportWriteResponse {
  // Operation result status code, 0 means success
  int32 status = 1;
}

// IndexType represents the type of index
enum IndexType {
  // Default value must be 0 in proto3
  UNKNOWN = 0;
  // Full-text index type
  FULL_TEXT = 1;
  // Custom index type
  CUSTOM = 2;
}

// ParserType represents the type of parser
enum ParserType {
  // Default value must be 0 in proto3
  UNKNOWN_PARSER = 0;
  // Default parser
  DEFAULT_PARSER = 1;
  // Other parser types
  OTHER_PARSER = 2;
}

message ShardManifestHeader {
  uint64 shard_id = 1;
  bytes start_key = 2;
  bytes end_key = 3;
  uint64 epoch = 4;
  uint64 seq = 5;
}

message S3Location {
  string bucket = 1;
  string prefix = 2;
}

// AddShardRequest is sent from meta service to worker node
message AddShardRequest {
  ShardManifestHeader shard = 1;
  // Index information
  IndexInfo index_info = 2;
  // Table information
  TableInfo table_info = 3;
  // If the shard is created from scratch, this field is empty
  string cdc_s3_last_file = 4;
  S3Location cdc_location = 5;
  S3Location upload_location = 6;
}

// AddShardResponse
message AddShardResponse {
  // Operation result status code, 0 means success
  int32 status = 1;
}

// CreateIndexRequest is a request to create an index
message CreateIndexRequest {
  string database_name = 1;
  bytes table_info = 2;
  int64 index_id = 3;
}

// CreateIndexResponse is a response to the index creation request
message CreateIndexResponse {
  ErrorCode status = 1;
  // Error message, only valid when status is non-zero
  string error_message = 2;
  // Created index ID
  string index_id = 3;
}

// DropIndexRequest is a request to drop an index
message DropIndexRequest {
  int64 table_id = 1;
  int64 index_id = 2;
}

// DropIndexResponse is a response to the index drop request
message DropIndexResponse {
  ErrorCode status = 1;
  // Error message, only valid when status is non-zero
  string error_message = 2;
}

// TableInfo represents table information
message TableInfo {
  // Table ID
  int64 table_id = 1;
  // Table name
  string table_name = 2;
  // Database name
  string database_name = 3;
  // Table version
  int64 version = 4;
  // Column information
  repeated ColumnInfo columns = 5;
  // Whether the table is clustered
  bool is_clustered = 6;
}

// ColumnInfo represents column information
message ColumnInfo {
  // Column ID
  int64 column_id = 1;
  // Column name
  string column_name = 2;
  // MySQL type
  int32 type = 3;
  // Collation
  int32 collation = 4;
  // Column length
  int32 column_length = 5;
  // Decimal places
  int32 decimal = 6;
  // Flags
  uint32 flag = 7;
  // Enum elements
  repeated string elems = 8;
  // Default value
  bytes default_val = 9;
  // Whether it's a primary key
  bool is_primary_key = 10;
  // Whether it's an array
  bool is_array = 11;
}

// IndexInfo represents index information
message IndexInfo {
  // Table ID
  int64 table_id = 1;
  // Index ID
  int64 index_id = 2;
  // Index name
  string index_name = 3;
  // Index type (fulltext, custom)
  IndexType index_type = 4;
  // Index columns
  repeated ColumnInfo columns = 5;
  // Whether the index is unique
  bool is_unique = 6;
  // Parser information
  ParserInfo parser_info = 7;
  // Other index parameters
  map<string, string> other_params = 8;
}

// ParserInfo represents parser information
message ParserInfo {
  // Parser type
  ParserType parser_type = 1;
  // Parser parameters
  map<string, string> parser_params = 2;
}

// GetIndexProgressRequest is a request to get the progress of an index build
message GetIndexProgressRequest {
  // Table ID
  int64 table_id = 1;
  // Index ID
  int64 index_id = 2;
}

// GetIndexProgressResponse is a response containing index build progress
message GetIndexProgressResponse {
  enum State {
    PENDING = 0;
    RUNNING = 1;
    COMPLETED = 2;
    FAILED = 3;
    NOTFOUND = 4;
    ERROR = 5;
  }
  ErrorCode status = 1;
  // Error message, only valid when status is non-zero
  string error_message = 2;
  // Number of documents indexed so far
  uint64 document_count = 3;
  // Build task state (PENDING, RUNNING, COMPLETED, FAILED)
  State state = 4;
  // Whether index has been uploaded to S3
  bool is_uploaded = 5;
  // Last S3 upload time in RFC3339 format
  string last_upload_time = 6;
  // S3 path where the index is stored
  string s3_path = 7;
}

// -----------------------------------------------------------------------------
// TiCI Full-text index data import workflow during Import Into (with DXF):
// -----------------------------------------------------------------------------
// 1. TiDB requests a cloud storage object prefix from the TiCI Meta Service.
//    It provides a unique tidb_task_id that specify a job in the TiDB side. A
//    job means one Import Into or Create Index operation.
//
// 2. Import DXF generate the files for partition data and upload them to the
//    cloud storage prefix returned by TiCI. The files should contain the
//    table info, indexes info and the key-value pairs within a specific
//    key-range.
//
// 3. After import DXF has uploaded one partition data belonging to a baseline
//    full text index, it calls `FinishImportPartitionUpload`. This allows
//    TiCI to mark the partition as complete and make it available to downstream
//    consumers.
//
// 4. After import DXF has uploaded all partitions data belonging to a baseline
//    full text index on a job, it calls `FinishImportIndexUpload`.
//    This allows TiCI to mark the job as complete and make it available to
//    GC data after consumed by downstream consumers.
// -----------------------------------------------------------------------------

message GetImportStoragePrefixRequest {
  // TiDB unique task ID for this Import Into/Create Index job.
  string tidb_task_id = 1;
  // Table ID of the target table.
  int64 table_id = 2;
  // Index ID of the target index. If this is an Import Into job that relates
  // to multiple indexes, this field should contain all the index IDs.
  repeated int64 index_ids = 3;
}

message GetImportStoragePrefixResponse {
  ErrorCode status = 1;
  // Optional human‑readable diagnostics, only defined when status is not
  // SUCCESS.
  string error_message = 2;
  // The import index job_id in TiCI
  uint64 job_id = 3;
  // The cloud storage prefix where the import data should be written.
  // The format is "s3://bucket/prefix/".
  string storage_uri = 4;
}

message FinishImportPartitionUploadRequest {
  // TiDB unique task ID for this Import Into/Create Index job.
  string tidb_task_id = 1;
  // Key range for the partition uploaded.
  KeyRange key_range = 2;
  // The cloud storage path where the partition data is written.
  // The format is "s3://bucket/prefix/filename".
  string storage_uri = 3;
}

// - If the index is built successfully, the `status` field should be 0,
//  and the `frag_metas` field should contain the metadata of the uploaded
//  baseline fragments. MetaService is responsible for
//  1. Find the overlapped Shards for the given table_id, index_id and key_range
//  2. Append the frags to all the overlapped shards' baseline frags
//  3. Update the ImportTask with task_id status as finished
// - If the index building meet unrecoverable errors, then MetaService should
//  update the ImportTask with task_id status as failed
message FinishImportPartitionBuildRequest {
  int64 table_id = 1;
  int64 index_id = 2;
  uint64 task_id = 3;
  ErrorCode status = 4;
  // The uploaded fragment metadata when status is 0.
  repeated FragMeta frag_metas = 5;
  // The error message if the status is not SUCCESS.
  string error_message = 6;
}

message FinishImportIndexUploadRequest {
  // TiDB unique task ID for this Import Into/Create Index job.
  string tidb_task_id = 1;
  // If status is SUCCESS, the job is finished successfully.
  // If status is not SUCCESS, the job is aborted or cancelled in TiDB side,
  // and TiCI should clean up all the related data.
  ErrorCode status = 2;
  // The error_message if the status is not SUCCESS.
  string error_message = 3;
}

message FinishImportResponse {
  ErrorCode status = 1;
  // Optional human‑readable diagnostics, only defined when status is not
  // SUCCESS.
  string error_message = 2;
}

// -----------------------------------------------------------------------------
