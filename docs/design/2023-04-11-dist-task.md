# Proposal: Unified distributed parallel execution framework

- Author: [Benjamin](https://github.com/Benjamin2037) 
- Tracking Issue:
## Background and benefits
TiDB is a distributed database with high scalability, available and performance on providing database service. Since v7.1,
The unified distributed parallel execution framework is introduced to gradually support all TiDB 
backend task(for example DDL, Import data, TTL, auto analyze and BR) execution in distributed way.
The framework may bring two outstanding benefits to TiDB user:
1. Distributed execution model could provide better scalability for backend tasks runtime performance.
2. It can achieve a better TiDB cluster resource utility for backend tasks.
3. It makes possibility that TiDB can do resource management with treating backend tasks as whole, and also schedule different tasks execution as user expectation.

## Goal and non-goal
Here we only mention the first version's goal and non-goal. The evolution of the framework will be described in the future work part.
### Goal
1. Build the fundamental component of the framework.
2. Support limited scenarios of backend tasks.
   1. For DDL 
      1. Support partition table create index in distributed way.
      2. Maybe support Normal table create index partially execution in distributed way. That means at v7.1 not full support normal table's create index in distributed way.
   2. For Import service: Support import table task executed in distributed way.
### Non-goal
1. Not support the query execution. The query distributed execution support, please see TiFlash relative docs.
2. Only provide fundamental support, and the framework will keep in evolution.
3. Not very sophistic HA capability, which will keep in being improved.
4. Not support the tasks runtime status, for example task execution progress etc., will provide in later release.
5. TTL, auto analyze and BR will be supported in later release.

** Note **
The release version is v7.1, what we say in later release means the Subsequent releases from v7.1.

## Architecture
This section we will describe the Architecture of the framework.
![disttaskflow.jpg](imgs%2Fdisttaskarchi.jpg)

From above architecture picture, we can see the below key modules to make the framework can handle incoming backend tasks run in distributed, with high performance and with good HA capability.
1. Global task queue:used to store the backend tasks runtime information.
2. Global subtask queue: used to store TiDB node level subtask runtime information. it is split from task handle data and is TiDB node level executor unit.
3. Dispatcher: is task runtime owner. it will do planner generate for tasks, monitor task runtime status and push task into next stage execution.
4. Scheduler: is TiDB node level subtask runtime owner. it will pick up one subtask and run it in one TiDB node in parallel ways, and will set subtask status and monitor task status, then determine what should do next.
5. Subtask executor: is the real worker to execute a slice of subtask and return result and err if there is any back to scheduler.
6. resource pool: For dispatcher, scheduler and executor, we limited them into pool according class. with introduce a global resource manager to limit the framework runtime resource usage quantity.

## Detail Design
In this section, the data structure and interface will be introduced to show user the detail design.
### system variable
A system variable is used to enable/disable the framework.
`tidb_enable_dist_task` 
```sql
mysql> set global tidb_enable_dist_task = ON|OFF;
```
### Data structure
In implementation, the table is used to represent the above two queue. below is the definition of above two queue.
`Global task queue`

| Name              | type                                           | Memo |
|-------------------|------------------------------------------------|------|
| id                | BIGINT(20) NOT NULL AUTO_INCREMENT PRIMARY KEY |      | 
| task_key          | VARCHAR(256) NOT NULL                          |      |
| type              | VARCHAR(256) NOT NULL                          |      |
| dispatcher_id     | VARCHAR(256)                                   |      |
| state             | VARCHAR(64) NOT NULL                           |      |
| start_time        | TIMESTAMP                                      |      |
| state_update_time | TIMESTAMP                                      |      |
| meta              | LONGBLOB                                       |      |
| concurrency       | INT(11)                                        |      |
| step              | INT(11)                                        |      |

`Global subtask queue`

| Name              | Type                                       | Memo |
|-------------------|--------------------------------------------|------|
| id                | bigint not null auto_increment primary key |      |
| namespace         | varchar(256)                               |      |
| task_key          | varchar(256)                               |      | 
| ddl_physical_tid  | bigint(20)                                 |      |
| type              | int                                        |      |  
| exec_id           | varchar(256)                               |      |
| exec_expired      | timestamp                                  |      |
| state             | varchar(64) not null                       |      |
| checkpoint        | longblob not null                          |      | 
| start_time        | bigint                                     |      |        
| state_update_time | bigint                                     |      | 
| meta              | longblob                                   |      |                                            |      |

### interface design
Dispatcher
```go
// Dispatch defines the interface for operations inside a dispatcher.
type Dispatch interface {
// Start enables dispatching and monitoring mechanisms.
Start()
// GetAllSchedulerIDs gets handles the task's all available instances.
GetAllSchedulerIDs(ctx context.Context, gTaskID int64) ([]string, error)
// Stop stops the dispatcher.
Stop()
}

type dispatcher struct {
	ctx     context.Context
	cancel  context.CancelFunc
	taskMgr *storage.TaskManager
	wg      tidbutil.WaitGroupWrapper
	gPool   *spool.Pool

	runningGTasks struct {
		syncutil.RWMutex
		taskIDs map[int64]struct{}
	}
	detectPendingGTaskCh chan *proto.Task
}
```
Scheduler
```go
// InternalScheduler defines the interface of an internal scheduler.
type InternalScheduler interface {
	Start()
	Stop()
	Run(context.Context, *proto.Task) error
	Rollback(context.Context, *proto.Task) error
}

// Scheduler defines the interface of a scheduler.
// User should implement this interface to define their own scheduler.
type Scheduler interface {
	InitSubtaskExecEnv(context.Context) error
	SplitSubtask(subtask []byte) []proto.MinimalTask
	CleanupSubtaskExecEnv(context.Context) error
	Rollback(context.Context) error
}

// InternalSchedulerImpl is the implementation of InternalScheduler.
type InternalSchedulerImpl struct {
ctx       context.Context
cancel    context.CancelFunc
id        string
taskID    int64
taskTable TaskTable
pool      Pool
wg        sync.WaitGroup
logCtx    context.Context

mu struct {
sync.RWMutex
err error
// runtimeCancel is used to cancel the Run/Rollback when error occurs.
runtimeCancel context.CancelFunc
}
}
```

## Usage
Currentlyï¼Œthe phase I unified distributed parallel execution framework will be an experimental feature released in v7.1.
The supported scenario is `Create index` or `Add index` task run in distributed ways.

For DDL tasks, there are below system variables and config parameter to enable distributed `Add index`.

**System Variables**:

* [tidb_ddl_reorg_worker_cnt](https://docs.pingcap.com/tidb/stable/system-variables#tidb_ddl_reorg_worker_cnt)
* [tidb_ddl_reorg_priority](https://docs.pingcap.com/tidb/stable/system-variables#tidb_ddl_reorg_priority)
* [tidb_ddl_error_count_limit](https://docs.pingcap.com/tidb/stable/system-variables#tidb_ddl_error_count_limit)
* [tidb_ddl_reorg_batch_size](https://docs.pingcap.com/tidb/stable/system-variables#tidb_ddl_reorg_batch_size)

**Fast DDL Path:**
* [tidb_ddl_enable_fast_reorg](https://docs.pingcap.com/tidb/stable/system-variables#tidb_ddl_enable_fast_reorg-new-in-v630)
* [tidb_ddl_disk_quota](https://docs.pingcap.com/tidb/stable/system-variables#tidb_ddl_disk_quota-new-in-v630)

**Config parameter:**
* [temp-dir](https://docs.pingcap.com/tidb/stable/tidb-configuration-file#temp-dir-new-in-v630)

## Future work
1. Stabilize the framework capability and make it handle task more smooth and friendly.
2. Support more backend tasks, for example: `Load Data`, `TTL`, `Auto Analyze` and `Backup|Restroe` etc.
3. Support pause and resume tasks. 
4. Support resource manager to control whole backend tasks runtime resource utility.
