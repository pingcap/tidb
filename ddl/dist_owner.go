// Copyright 2023 PingCAP, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package ddl

import (
	"context"
	"encoding/hex"
	"fmt"
	"strconv"
	"time"

	"github.com/pingcap/errors"
	"github.com/pingcap/tidb/kv"
	"github.com/pingcap/tidb/meta"
	"github.com/pingcap/tidb/metrics"
	"github.com/pingcap/tidb/parser/model"
	"github.com/pingcap/tidb/parser/mysql"
	"github.com/pingcap/tidb/sessionctx"
	"github.com/pingcap/tidb/sessionctx/variable"
	"github.com/pingcap/tidb/table"
	tidbutil "github.com/pingcap/tidb/util"
	"github.com/pingcap/tidb/util/dbterror"
	"github.com/pingcap/tidb/util/logutil"
	"github.com/pingcap/tidb/util/mathutil"
	atomicutil "go.uber.org/atomic"
	"go.uber.org/zap"
)

// CheckBackfillJobFinishInterval is export for test.
var (
	CheckBackfillJobFinishInterval = 300 * time.Millisecond
	telemetryDistReorgUsage        = metrics.TelemetryDistReorgCnt
)

const (
	distPhysicalTableConcurrency = 16
)

func initDistReorg(reorgMeta *model.DDLReorgMeta) {
	isDistReorg := variable.DDLEnableDistributeReorg.Load()
	reorgMeta.IsDistReorg = isDistReorg
	if isDistReorg {
		metrics.TelemetryDistReorgCnt.Inc()
	}
}

// BackfillJobRangeMeta is export for test.
type BackfillJobRangeMeta struct {
	ID       int64
	PhyTblID int64
	PhyTbl   table.PhysicalTable
	StartKey []byte
	EndKey   []byte
}

func (m *BackfillJobRangeMeta) String() string {
	physicalID := strconv.FormatInt(m.PhyTblID, 10)
	startKey := hex.EncodeToString(m.StartKey)
	endKey := hex.EncodeToString(m.EndKey)
	rangeStr := "taskID_" + strconv.Itoa(int(m.ID)) + "_physicalTableID_" + physicalID + "_" + "[" + startKey + "," + endKey + ")"
	return rangeStr
}

type splitJobContext struct {
	ctx               context.Context
	cancel            context.CancelFunc
	isMultiPhyTbl     bool
	bfWorkerType      backfillerType
	isUnique          bool
	batchSize         int
	minBatchSize      int
	currBackfillJobID *atomicutil.Int64
	currPhysicalID    int64
	phyTblMetaCh      chan *BackfillJobRangeMeta
	resultCh          chan error
}

func getRunningPhysicalTableMetas(sess *session, sJobCtx *splitJobContext, reorgInfo *reorgInfo) ([]*BackfillJobRangeMeta, error) {
	ddlJobID, eleID, eleKey, currPID := reorgInfo.Job.ID, reorgInfo.currElement.ID, reorgInfo.currElement.TypeKey, reorgInfo.PhysicalTableID
	pTblMetas, err := GetPhysicalTableMetas(sess, ddlJobID, eleID, eleKey)
	if err != nil {
		return nil, errors.Trace(err)
	}

	currBfJobID := int64(1)
	physicalTIDs := make([]int64, 0, len(pTblMetas))
	phyTblMetas := make([]*BackfillJobRangeMeta, 0, len(pTblMetas))
	if len(pTblMetas) == 0 {
		bfJM := &BackfillJobRangeMeta{PhyTblID: currPID, StartKey: reorgInfo.StartKey, EndKey: reorgInfo.EndKey}
		phyTblMetas = append(phyTblMetas, bfJM)
		physicalTIDs = append(physicalTIDs, bfJM.PhyTblID)
	} else {
		for _, pMeta := range pTblMetas {
			phyTblMetas = append(phyTblMetas, pMeta)
			currPID = mathutil.Max(pMeta.PhyTblID, currPID)
			currBfJobID = mathutil.Max(pMeta.ID, currBfJobID)
			physicalTIDs = append(physicalTIDs, pMeta.PhyTblID)
		}
	}
	sJobCtx.currPhysicalID = currPID
	sJobCtx.currBackfillJobID = atomicutil.NewInt64(currBfJobID)
	logutil.BgLogger().Info("[ddl] unprocessed physical table ranges get from table", zap.Int64("jobID", ddlJobID),
		zap.Int64("eleID", eleID), zap.ByteString("eleKey", eleKey),
		zap.Int64("currPID", sJobCtx.currPhysicalID), zap.Int64s("phyTblIDs", physicalTIDs))
	return phyTblMetas, nil
}

func (dc *ddlCtx) sendPhysicalTableMetas(reorgInfo *reorgInfo, t table.Table, sJobCtx *splitJobContext, runningPTblMetas []*BackfillJobRangeMeta) {
	var err error
	physicalTIDs := make([]int64, 0, distPhysicalTableConcurrency)
	defer func() {
		logutil.BgLogger().Info("[ddl] send physical table ranges to split finished", zap.Int64("jobID", reorgInfo.Job.ID),
			zap.Stringer("ele", reorgInfo.currElement), zap.Int64s("phyTblIDs", physicalTIDs), zap.Error(err))
		if err != nil {
			sJobCtx.cancel()
		} else {
			close(sJobCtx.phyTblMetaCh)
		}
	}()

	for _, pTblM := range runningPTblMetas {
		err = dc.isReorgRunnable(reorgInfo.Job.ID, false)
		if err != nil {
			return
		}

		if tbl, ok := t.(table.PartitionedTable); ok {
			pTblM.PhyTbl = tbl.GetPartition(pTblM.PhyTblID)
			sJobCtx.phyTblMetaCh <- pTblM
		} else {
			//nolint:forcetypeassert
			phyTbl := t.(table.PhysicalTable)
			pTblM.PhyTbl = phyTbl
			sJobCtx.phyTblMetaCh <- pTblM
		}
		physicalTIDs = append(physicalTIDs, pTblM.PhyTblID)
	}

	if tbl, ok := t.(table.PartitionedTable); ok {
		currPhysicalID := sJobCtx.currPhysicalID
		for {
			err = dc.isReorgRunnable(reorgInfo.Job.ID, false)
			if err != nil {
				return
			}
			select {
			case <-sJobCtx.ctx.Done():
				err = sJobCtx.ctx.Err()
				return
			default:
			}

			pID, startKey, endKey, err1 := getNextPartitionInfo(reorgInfo, tbl, currPhysicalID)
			if err1 != nil {
				err = err1
				return
			}
			if pID == 0 {
				// Next partition does not exist, all the job done.
				return
			}
			pTbl := tbl.GetPartition(pID)
			if pTbl == nil {
				err = dbterror.ErrCancelledDDLJob.GenWithStack("Can not find partition id %d for table %d", pID, t.Meta().ID)
				return
			}
			bfJM := &BackfillJobRangeMeta{PhyTblID: pID, PhyTbl: pTbl, StartKey: startKey, EndKey: endKey}
			sJobCtx.phyTblMetaCh <- bfJM
			currPhysicalID = pID

			physicalTIDs = append(physicalTIDs, pID)
		}
	}
}

func (dc *ddlCtx) controlWriteTableRecord(sessPool *sessionPool, t table.Table, bfWorkerType backfillerType, reorgInfo *reorgInfo) error {
	startKey, endKey := reorgInfo.StartKey, reorgInfo.EndKey
	if startKey == nil && endKey == nil {
		return nil
	}

	ddlJobID := reorgInfo.Job.ID
	currEle := reorgInfo.currElement
	logutil.BgLogger().Info("[ddl] control write table record start",
		zap.Int64("jobID", ddlJobID), zap.Stringer("ele", currEle),
		zap.Int64("tblID", t.Meta().ID), zap.Int64("currPID", reorgInfo.PhysicalTableID))
	sCtx, err := sessPool.get()
	if err != nil {
		return errors.Trace(err)
	}
	defer sessPool.put(sCtx)
	sess := newSession(sCtx)

	if err := dc.isReorgRunnable(ddlJobID, true); err != nil {
		return errors.Trace(err)
	}
	var isUnique bool
	if bfWorkerType == typeAddIndexWorker {
		idxInfo := model.FindIndexInfoByID(t.Meta().Indices, currEle.ID)
		isUnique = idxInfo.Unique
	}

	wg := tidbutil.WaitGroupWrapper{}
	sJobCtx := &splitJobContext{
		bfWorkerType: bfWorkerType,
		isUnique:     isUnique,
		batchSize:    genTaskBatch,
		minBatchSize: minGenTaskBatch,
		phyTblMetaCh: make(chan *BackfillJobRangeMeta, 1),
		resultCh:     make(chan error, distPhysicalTableConcurrency),
	}
	ctx := kv.WithInternalSourceType(context.Background(), kv.InternalTxnDDL)
	sJobCtx.ctx, sJobCtx.cancel = context.WithCancel(ctx)
	concurrency := 1
	if tbl, ok := t.(table.PartitionedTable); ok {
		ids := len(tbl.GetAllPartitionIDs())
		if ids > 1 {
			sJobCtx.isMultiPhyTbl = true
			concurrency = ids
		}
		if ids > distPhysicalTableConcurrency {
			concurrency = distPhysicalTableConcurrency
		}
		sJobCtx.batchSize = genPhysicalTableTaskBatch
		sJobCtx.minBatchSize = minGenPhysicalTableTaskBatch
	}

	err = checkAndHandleInterruptedBackfillJobs(sess, ddlJobID, currEle.ID, currEle.TypeKey)
	if err != nil {
		return errors.Trace(err)
	}
	phyTblMetas, err := getRunningPhysicalTableMetas(sess, sJobCtx, reorgInfo)
	if err != nil {
		return err
	}

	sCtxs := make([]sessionctx.Context, 0, concurrency)
	for i := 0; i < concurrency; i++ {
		sCtx, err := sessPool.get()
		if err != nil {
			return err
		}
		sCtxs = append(sCtxs, sCtx)
	}

	wg.Run(func() {
		defer tidbutil.Recover(metrics.LabelDistReorg, "sendPhysicalTableMeta", nil, false)
		dc.sendPhysicalTableMetas(reorgInfo, t, sJobCtx, phyTblMetas)
	})
	for _, sCtx := range sCtxs {
		func(ctx sessionctx.Context) {
			wg.Run(func() {
				defer func() {
					tidbutil.Recover(metrics.LabelDistReorg, "splitTableToBackfillJobs", nil, false)
				}()
				se := newSession(ctx)
				dc.splitPhysicalTableToBackfillJobs(se, reorgInfo, sJobCtx)
			})
		}(sCtx)
	}
	wg.Wait()
	for _, sCtx := range sCtxs {
		sessPool.put(sCtx)
	}
	return checkReorgJobFinished(dc.ctx, sess, &dc.reorgCtx, ddlJobID, currEle)
}

func (dc *ddlCtx) splitPhysicalTableToBackfillJobs(sess *session, reorgInfo *reorgInfo, sJobCtx *splitJobContext) {
	defaultSQLMode := sess.GetSessionVars().SQLMode
	defer func() { sess.GetSessionVars().SQLMode = defaultSQLMode }()
	// Make timestamp type can be inserted ZeroTimestamp.
	sess.GetSessionVars().SQLMode = mysql.ModeNone

	var err error
	var pTblMetaCnt int
	var pTblMeta *BackfillJobRangeMeta
	defer func() {
		if err != nil {
			sJobCtx.cancel()
		}
		logutil.BgLogger().Info("[ddl] split backfill jobs to table finish", zap.Int64("jobID", reorgInfo.Job.ID),
			zap.Stringer("ele", reorgInfo.currElement), zap.Int("donePTbls", pTblMetaCnt), zap.Stringer("physical_tbl", pTblMeta), zap.Error(err))
	}()

	var ok bool
	for {
		select {
		case <-sJobCtx.ctx.Done():
			err = sJobCtx.ctx.Err()
		case pTblMeta, ok = <-sJobCtx.phyTblMetaCh:
			if !ok {
				return
			}
			if err = dc.isReorgRunnable(reorgInfo.Job.ID, false); err != nil {
				return
			}

			err = dc.splitTableToBackfillJobs(sess, reorgInfo, sJobCtx, pTblMeta)
			if err != nil {
				return
			}
			pTblMetaCnt++
		}
	}
}

func checkReorgJobFinished(ctx context.Context, sess *session, reorgCtxs *reorgContexts, ddlJobID int64, currEle *meta.Element) error {
	var times int64
	var bfJob *BackfillJob
	var backfillJobFinished bool
	ticker := time.NewTicker(CheckBackfillJobFinishInterval)
	defer ticker.Stop()
	for {
		if getReorgCtx(reorgCtxs, ddlJobID).isReorgCanceled() {
			// Job is cancelled. So it can't be done.
			return dbterror.ErrCancelledDDLJob
		}

		select {
		case <-ticker.C:
			times++
			// Print this log every 5 min.
			if times%1000 == 0 {
				logutil.BgLogger().Info("[ddl] check all backfill jobs is finished",
					zap.Int64("job ID", ddlJobID), zap.Bool("isFinished", backfillJobFinished), zap.Reflect("bfJob", bfJob))
			}
			if !backfillJobFinished {
				err := checkAndHandleInterruptedBackfillJobs(sess, ddlJobID, currEle.ID, currEle.TypeKey)
				if err != nil {
					logutil.BgLogger().Warn("[ddl] finish interrupted backfill jobs", zap.Int64("job ID", ddlJobID), zap.Stringer("ele", currEle), zap.Error(err))
					return errors.Trace(err)
				}

				bfJobs, err := getBackfillJobWithRetry(sess, BackfillTable, ddlJobID, currEle.ID, currEle.TypeKey)
				if err != nil {
					logutil.BgLogger().Info("[ddl] getBackfillJobWithRetry failed", zap.Int64("job ID", ddlJobID), zap.Stringer("ele", currEle), zap.Error(err))
					return errors.Trace(err)
				}
				if len(bfJobs) == 0 {
					backfillJobFinished = true
					logutil.BgLogger().Info("[ddl] finish all backfill jobs", zap.Int64("job ID", ddlJobID), zap.Stringer("ele", currEle))
				}
			}
			if backfillJobFinished {
				// TODO: Consider whether these backfill jobs are always out of sync.
				isSynced, err := checkJobIsFinished(sess, ddlJobID)
				if err != nil {
					logutil.BgLogger().Warn("[ddl] checkJobIsFinished failed", zap.Int64("job ID", ddlJobID), zap.Stringer("ele", currEle), zap.Error(err))
					return errors.Trace(err)
				}
				if isSynced {
					logutil.BgLogger().Info("[ddl] finish all backfill jobs and put them to history", zap.Int64("job ID", ddlJobID), zap.Stringer("ele", currEle))
					return GetBackfillErr(sess, ddlJobID, currEle.ID, currEle.TypeKey)
				}
			}
		case <-ctx.Done():
			return ctx.Err()
		}
	}
}

func checkJobIsFinished(sess *session, ddlJobID int64) (bool, error) {
	var err error
	var unsyncedInstanceIDs []string
	for i := 0; i < retrySQLTimes; i++ {
		unsyncedInstanceIDs, err = getUnsyncedInstanceIDs(sess, ddlJobID, "check_backfill_history_job_sync")
		if err == nil && len(unsyncedInstanceIDs) == 0 {
			return true, nil
		}

		logutil.BgLogger().Info("[ddl] checkJobIsSynced failed",
			zap.Strings("unsyncedInstanceIDs", unsyncedInstanceIDs), zap.Int("tryTimes", i), zap.Error(err))
		time.Sleep(RetrySQLInterval)
	}

	return false, errors.Trace(err)
}

// GetBackfillErr gets the error in backfill job.
func GetBackfillErr(sess *session, ddlJobID, currEleID int64, currEleKey []byte) error {
	var err error
	var metas []*model.BackfillMeta
	for i := 0; i < retrySQLTimes; i++ {
		metas, err = GetBackfillMetas(sess, BackfillHistoryTable, fmt.Sprintf("ddl_job_id = %d and ele_id = %d and ele_key = %s",
			ddlJobID, currEleID, wrapKey2String(currEleKey)), "get_backfill_job_metas")
		if err == nil {
			for _, m := range metas {
				if m.Error != nil {
					return m.Error
				}
			}
			return nil
		}

		logutil.BgLogger().Info("[ddl] GetBackfillMetas failed in checkJobIsSynced", zap.Int("tryTimes", i), zap.Error(err))
		time.Sleep(RetrySQLInterval)
	}

	return errors.Trace(err)
}

func checkAndHandleInterruptedBackfillJobs(sess *session, ddlJobID, currEleID int64, currEleKey []byte) (err error) {
	var bJobs []*BackfillJob
	for i := 0; i < retrySQLTimes; i++ {
		bJobs, err = GetInterruptedBackfillJobForOneEle(sess, ddlJobID, currEleID, currEleKey)
		if err == nil {
			break
		}
		logutil.BgLogger().Info("[ddl] getInterruptedBackfillJobsForOneEle failed", zap.Error(err))
		time.Sleep(RetrySQLInterval)
	}
	if err != nil {
		return errors.Trace(err)
	}
	if len(bJobs) == 0 {
		return nil
	}

	for i := 0; i < retrySQLTimes; i++ {
		err = MoveBackfillJobsToHistoryTable(sess, bJobs[0])
		if err == nil {
			return bJobs[0].Meta.Error
		}
		logutil.BgLogger().Info("[ddl] MoveBackfillJobsToHistoryTable failed", zap.Error(err))
		time.Sleep(RetrySQLInterval)
	}
	return errors.Trace(err)
}

func checkBackfillJobCount(sess *session, ddlJobID, currEleID int64, currEleKey []byte, pTblID int64) (backfillJobCnt int, err error) {
	err = checkAndHandleInterruptedBackfillJobs(sess, ddlJobID, currEleID, currEleKey)
	if err != nil {
		return 0, errors.Trace(err)
	}

	backfillJobCnt, err = GetBackfillJobCount(sess, BackfillTable,
		fmt.Sprintf("ddl_job_id = %d and ele_id = %d and ele_key = %s and ddl_physical_tid = %d",
			ddlJobID, currEleID, wrapKey2String(currEleKey), pTblID), "check_backfill_job_count")
	if err != nil {
		return 0, errors.Trace(err)
	}

	return backfillJobCnt, nil
}

func getBackfillJobWithRetry(sess *session, tableName string, ddlJobID, currEleID int64, currEleKey []byte) ([]*BackfillJob, error) {
	var err error
	var bJobs []*BackfillJob
	for i := 0; i < retrySQLTimes; i++ {
		bJobs, err = GetBackfillJobs(sess, tableName, fmt.Sprintf("ddl_job_id = %d and ele_id = %d and ele_key = %s limit 1",
			ddlJobID, currEleID, wrapKey2String(currEleKey)), "check_backfill_job_state")
		if err != nil {
			logutil.BgLogger().Warn("[ddl] GetBackfillJobs failed", zap.Error(err))
			time.Sleep(RetrySQLInterval)
			continue
		}

		return bJobs, nil
	}
	return nil, errors.Trace(err)
}

// GetPhysicalTableMetas gets the max backfill metas per physical table in BackfillTable and BackfillHistoryTable.
func GetPhysicalTableMetas(sess *session, ddlJobID, currEleID int64, currEleKey []byte) (map[int64]*BackfillJobRangeMeta, error) {
	condition := fmt.Sprintf("ddl_job_id = %d and ele_id = %d and ele_key = %s", ddlJobID, currEleID, wrapKey2String(currEleKey))
	pTblMs, err := GetBackfillIDAndMetas(sess, BackfillTable, condition, "get_ptbl_metas")
	if err != nil {
		return nil, errors.Trace(err)
	}
	hPTblMs, err := GetBackfillIDAndMetas(sess, BackfillHistoryTable, condition, "get_ptbl_metas")
	if err != nil {
		return nil, errors.Trace(err)
	}

	metaMap := make(map[int64]*BackfillJobRangeMeta, len(pTblMs)+len(hPTblMs))
	for _, m := range pTblMs {
		metaMap[m.PhyTblID] = m
	}
	for _, m := range hPTblMs {
		val, ok := metaMap[m.PhyTblID]
		if !ok || (ok && m.ID > val.ID) {
			metaMap[m.PhyTblID] = m
		}
	}
	return metaMap, nil
}

// MoveBackfillJobsToHistoryTable moves backfill table jobs to the backfill history table.
func MoveBackfillJobsToHistoryTable(sctx sessionctx.Context, bfJob *BackfillJob) error {
	s, ok := sctx.(*session)
	if !ok {
		return errors.Errorf("sess ctx:%#v convert session failed", sctx)
	}

	return s.runInTxn(func(se *session) error {
		// TODO: Consider batch by batch update backfill jobs and insert backfill history jobs.
		bJobs, err := GetBackfillJobs(se, BackfillTable, fmt.Sprintf("ddl_job_id = %d and ele_id = %d and ele_key = %s",
			bfJob.JobID, bfJob.EleID, wrapKey2String(bfJob.EleKey)), "update_backfill_job")
		if err != nil {
			return errors.Trace(err)
		}
		if len(bJobs) == 0 {
			return nil
		}

		txn, err := se.txn()
		if err != nil {
			return errors.Trace(err)
		}
		startTS := txn.StartTS()
		err = RemoveBackfillJob(se, true, bJobs[0])
		if err == nil {
			for _, bj := range bJobs {
				bj.State = model.JobStateCancelled
				bj.FinishTS = startTS
			}
			err = AddBackfillHistoryJob(se, bJobs)
		}
		logutil.BgLogger().Info("[ddl] move backfill jobs to history table", zap.Int("job count", len(bJobs)))
		return errors.Trace(err)
	})
}
